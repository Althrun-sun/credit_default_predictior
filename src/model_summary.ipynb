{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e39fd6-56bb-429f-868a-6699d14b7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0dadc3-a78c-433a-b73c-137345dcbcce",
   "metadata": {},
   "source": [
    "- For each model I need:\n",
    "    * ~~**[optional]** corss validation results -> different scores~~\n",
    "    * confusion matrix\n",
    "    * precision recall curve\n",
    "    * ROC AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b115e2-0626-4fd4-8439-4ffa52052f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: read models from file -> dc\n",
    "# TODO: read training data from file -> X_train, y_train\n",
    "df = pd.read_csv(\"data/bigml_59c28831336c6604c800002a.csv\", encoding=\"utf-8\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=123)\n",
    "X_train = train_df.drop(columns=[\"churn\"])\n",
    "X_test = test_df.drop(columns=[\"churn\"])\n",
    "y_train = train_df[\"churn\"]\n",
    "y_test = test_df[\"churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adecc39-84e5-40f3-901d-f2fc3c76a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_results is a dataframe with mean and sd of CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ea61c-ec67-41f0-b6f7-b3508eca66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "classification_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "# The dummy model\n",
    "dc_0 = DummyClassifier()\n",
    "dc_0.fit(X_train, y_train)\n",
    "dc_1 = DummyClassifier()\n",
    "dc_1.fit(X_train, y_train)\n",
    "dc_2 = DummyClassifier()\n",
    "dc_2.fit(X_train, y_train)\n",
    "dc_3 = DummyClassifier()\n",
    "dc_3.fit(X_train, y_train)\n",
    "\n",
    "# The mean and std of the cross validated scores for all metrics as a dataframe\n",
    "cross_val_results = {}\n",
    "\n",
    "cross_val_results['dummy_0'] = pd.DataFrame(\n",
    "    cross_validate(dc_0, X_train, y_train, return_train_score=True, scoring=classification_metrics)\n",
    ").agg(['mean', 'std']).round(3).T\n",
    "\n",
    "cross_val_results['dummy_1'] = pd.DataFrame(\n",
    "    cross_validate(dc_1, X_train, y_train, return_train_score=True, scoring=classification_metrics)\n",
    ").agg(['mean', 'std']).round(3).T\n",
    "\n",
    "cross_val_results['dummy_2'] = pd.DataFrame(\n",
    "    cross_validate(dc_2, X_train, y_train, return_train_score=True, scoring=classification_metrics)\n",
    ").agg(['mean', 'std']).round(3).T\n",
    "\n",
    "cross_val_results['dummy_3'] = pd.DataFrame(\n",
    "    cross_validate(dc_3, X_train, y_train, return_train_score=True, scoring=classification_metrics)\n",
    ").agg(['mean', 'std']).round(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebdc0d-5d6d-4d47-b37f-dc0b515ad7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the average scores of all the models\n",
    "pd.concat(\n",
    "    cross_val_results,\n",
    "    axis='columns'\n",
    ").xs(\n",
    "    'mean',\n",
    "    axis='columns',\n",
    "    level=1\n",
    ").style.format(\n",
    "    precision=2\n",
    ").background_gradient(\n",
    "    axis=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8247e14-59b2-4249-ac5b-bd6046b8ca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that the cross val std doesn't look way off for some model\n",
    "pd.concat(\n",
    "    cross_val_results,\n",
    "    axis='columns'  # Get the right model names and mean/std as columns\n",
    ").xs(\n",
    "    'std',  # Select only the 'std' columns\n",
    "    axis='columns',  # Cross-section the columns\n",
    "    level=1  # The 1st level ('mean', 'std') instead of the 0th level (the model names)\n",
    ").style.format(\n",
    "    precision=2  # Pandas `.style` does not honor previous rounding via `.round()`\n",
    ").background_gradient(\n",
    "    axis=None  # Color cells based on the entire matrix rather than row/column-wise\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a960db1-2060-4c91-9f92-c0aa1b313303",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_dc_0 = confusion_matrix(y_test, dc_0.predict(X_test))\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix_dc_0, display_labels=[\"Non Default\", \"Default\"]\n",
    ").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded7cea5-3a11-4992-98f7-bbd41a566364",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, dc_0.predict_proba(X_test)[:, 1]\n",
    ")\n",
    "plt.plot(precision, recall, label=\"dc_0: PR curve\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.plot(\n",
    "    precision_score(y_test, dc_0.predict(X_test)),\n",
    "    recall_score(y_test, dc_0.predict(X_test)),\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e3b32-18ef-409b-b8d7-120dd5a31a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, dc_0.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "\n",
    "default_threshold = np.argmin(np.abs(thresholds - 0.5))\n",
    "\n",
    "plt.plot(\n",
    "    fpr[default_threshold],\n",
    "    tpr[default_threshold],\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:522]",
   "language": "python",
   "name": "conda-env-522-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
